#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŒèŠ±é¡ºå¿«é€Ÿæ•°æ®é‡‡é›†è„šæœ¬
å‘½ä»¤è¡Œå·¥å…·ï¼Œå¿«é€Ÿè·å–æ•°æ®
"""

import sys
import json
import argparse
from datetime import datetime


def main():
    parser = argparse.ArgumentParser(
        description='åŒèŠ±é¡ºæ•°æ®é‡‡é›†å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  python quick_crawl.py --code 600519           # è·å–èŒ…å°å®æ—¶ä»·æ ¼
  python quick_crawl.py --codes 600519 000001   # è·å–å¤šåªè‚¡ç¥¨
  python quick_crawl.py --index                 # è·å–å¤§ç›˜æŒ‡æ•°
  python quick_crawl.py --monitor 600519        # ç›‘æ§è‚¡ç¥¨ä»·æ ¼
  python quick_crawl.py --export 600519 600036  # å¯¼å‡ºCSV
        '''
    )
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--code', '-c', help='è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--codes', '-C', nargs='+', help='å¤šä¸ªè‚¡ç¥¨ä»£ç ')
    parser.add_argument('--index', '-i', action='store_true', help='è·å–å¤§ç›˜æŒ‡æ•°')
    parser.add_argument('--monitor', '-m', nargs='+', help='ç›‘æ§è‚¡ç¥¨')
    parser.add_argument('--export', '-e', nargs='+', help='å¯¼å‡ºCSV')
    parser.add_argument('--output', '-o', default='json', choices=['json', 'csv'], help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--interval', '-t', type=int, default=60, help='ç›‘æ§é—´éš”(ç§’)')
    parser.add_argument('--count', '-n', type=int, default=10, help='é‡‡é›†æ¬¡æ•°')
    
    args = parser.parse_args()
    
    # å¯¼å…¥æµè§ˆå™¨æ§åˆ¶
    from openclaw_integration import OpenClawBrowser
    from ths_crawler import THSCrawler
    from advanced_crawler import THSAdvancedCrawler
    
    print("=" * 60)
    print("  åŒèŠ±é¡ºæ•°æ®é‡‡é›†å·¥å…·")
    print("=" * 60)
    print()
    
    # åˆå§‹åŒ–
    browser = OpenClawBrowser()
    simple_crawler = THSCrawler(browser)
    advanced_crawler = THSAdvancedCrawler(browser)
    
    # æ£€æŸ¥è¿æ¥
    status = browser.status()
    if not status.get('success'):
        print("âŒ æ— æ³•è¿æ¥åˆ°æµè§ˆå™¨æ§åˆ¶æœåŠ¡å™¨")
        print("   è¯·ç¡®ä¿: 1) Chromeæ‰©å±•å·²å®‰è£… 2) server.pyå·²å¯åŠ¨")
        sys.exit(1)
    
    print(f"âœ… å·²è¿æ¥åˆ°æµè§ˆå™¨æ§åˆ¶æœåŠ¡å™¨")
    print()
    
    # æ‰§è¡Œä»»åŠ¡
    if args.monitor:
        # ç›‘æ§æ¨¡å¼
        print(f"ğŸ”„ å¼€å§‹ç›‘æ§ {len(args.monitor)} åªè‚¡ç¥¨...")
        advanced_crawler.monitor_prices(
            args.monitor,
            interval=args.interval,
            max_iterations=args.count
        )
    
    elif args.codes:
        # æ‰¹é‡æŸ¥è¯¢
        print(f"ğŸ“Š æŸ¥è¯¢ {len(args.codes)} åªè‚¡ç¥¨...")
        results = advanced_crawler.compare_stocks(args.codes)
        
        for result in results:
            code = result.get('code', result.get('data', {}).get('code', 'Unknown'))
            price = result.get('data', {}).get('price', 'N/A')
            change = result.get('data', {}).get('change_percent', 'N/A')
            print(f"  {code}: {price} ({change})")
        
        # å¯¼å‡º
        if args.output == 'csv':
            filepath = advanced_crawler.save_to_csv(results, 'stock_comparison')
            print(f"\nğŸ“ æ•°æ®å·²å¯¼å‡ºåˆ°: {filepath}")
    
    elif args.code:
        # å•åªæŸ¥è¯¢
        print(f"ğŸ“ˆ è·å– {args.code} å®æ—¶æ•°æ®...")
        result = advanced_crawler.get_stock_realtime_data(args.code)
        
        if result.get('success'):
            data = result.get('data', {})
            print(f"""
  ä»£ç : {data.get('code')}
  ä»·æ ¼: {data.get('price')}
  æ¶¨è·Œ: {data.get('change')} ({data.get('change_percent')})
  å¼€ç›˜: {data.get('open')}
  æœ€é«˜: {data.get('high')}
  æœ€ä½: {data.get('low')}
  æ˜¨æ”¶: {data.get('pre_close')}
  æˆäº¤é‡: {data.get('volume')}
  æˆäº¤é¢: {data.get('amount')}
            """)
            
            # å¯¼å‡º
            if args.output == 'csv':
                filepath = advanced_crawler.save_to_csv([result], 'stock_price')
                print(f"ğŸ“ æ•°æ®å·²å¯¼å‡ºåˆ°: {filepath}")
        else:
            print(f"âŒ è·å–å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    elif args.index:
        # å¤§ç›˜æŒ‡æ•°
        print("ğŸ“Š è·å–å¤§ç›˜æŒ‡æ•°...")
        result = advanced_crawler.get_market_index()
        
        if result.get('success'):
            indices = result.get('market', {})
            for key, data in indices.items():
                print(f"  {data.get('name', key)}: {data.get('price', 'N/A')} ({data.get('change', 'N/A')})")
        else:
            print("âŒ è·å–å¤±è´¥")
    
    elif args.export:
        # å¯¼å‡ºCSV
        print(f"ğŸ“ å¯¼å‡º {len(args.export)} åªè‚¡ç¥¨æ•°æ®åˆ°CSV...")
        results = advanced_crawler.compare_stocks(args.export)
        filepath = advanced_crawler.save_to_csv(results, 'export')
        print(f"âœ… å·²å¯¼å‡ºåˆ°: {filepath}")
    
    else:
        parser.print_help()
    
    print()
    print("=" * 60)


if __name__ == '__main__':
    main()
